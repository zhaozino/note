1、高吞吐量/低延时
	虽然kafka会持久化所有数据到磁盘，但本质上每次写入都只是把数据写入到操作系统的页缓存（page cache）中，然后由操作系统自行决定什么时候把
		页缓存中的数据写回磁盘。
	
	这样设计的优势：
		1、页缓存时在内存中分配的，所以写入速度非常快；
		2、kafka不必与底层的文件系统打交道，所有繁琐的I/O操作都交给操作系统处理；
		3、kafka写入操作采用追加写入（append）的方式，避免了磁盘随机写操作；
		4、日志压缩，发送压缩；
		
	kafka在读取消息时首先从页缓存中读，如果命中，便把消息直接发送到网络的socket上，这个过程就是利用linux平台的sendfile系统调用做到的，
		即：零拷贝技术（Zero Copy）；
	
	零拷贝：传统的Linux操作系统中的I/O接口是依托于数据拷贝来实现的，一个I/O操作会将同一份数据多次拷贝，数据传输过程中还涉及内核态与用户态
		的上下文切换，CPU的开销非常大，极大地限制了OS高效进行数据传输的能力。
		零拷贝技术：在内核驱动程序处理I/O数据的时候，它不再需要进行上下文的切换，节省了内核缓冲区与用户态应用程序缓冲区之间的数据拷贝，
		同时，它利用直接存储器访问技术（Direct Memory Access，DMA）执行I/O操作，因此也避免了OS内核缓冲区之间的数据拷贝，即零拷贝；
		linux提供的sendfile系统调用实现了这种零拷贝技术，而kafka的消息消费机制使用的就是sendfile，通过java的FileChannel.transferTo方法实现的；
	
	Kafka server读消息传送到消费者
	传统：硬盘—>内核buf—>用户buf—>socket相关缓冲区—>协议引擎
	sendfile：硬盘->内核buf->socket相关缓冲区->协议引擎
	
传统IO|缓存IO
	传统IO也就是缓存IO。数据先从磁盘复制到内核空间缓冲区，然后从内核空间缓冲区复制到应用程序的地址空间。这里的内核缓冲区也就是页缓存-PageCache，是虚拟内存空间
	读操作：操作系统检查内核的缓冲区有没有需要的数据，如果已经缓存了，那么就直接从缓存中返回；否则从磁盘中读取，然后缓存在操作系统的缓存中
	写操作：将数据从用户空间复制到内核空间的缓存中。这时对用户程序来说写操作就已经完成，至于什么时候再写到磁盘中由操作系统决定，
		除非显示地调用了sync同步命令：sync、fsync与fdatasync
	优点：分离了内核空间和用户空间，保护系统本身的运行安全；减少读盘次数，提升性能
	缺点：在缓存I/O机制中，DMA方式可以将数据直接从磁盘读到页缓存中，或者将数据从页缓存直接写回到磁盘上，而不能直接在应用程序地址空间和磁盘之间进行数据传输。
		这样，数据在传输过程中需要在应用程序地址空间（用户空间）和缓存（内核空间）之间进行多次数据拷贝操作，这些数据拷贝操作所带来的CPU以及内存开销是非常大的。
	
零拷贝
	Linux零拷贝分为：直接io、mmap()、sendfile()、splice()

直接IO：应用程序直接访问磁盘数据，而不经过内核缓冲区。这样做的目的是减少一次从内核缓冲区到用户程序缓存的数据复制。比如说数据库管理系统这类应用，
		它们更倾向于选择它们自己的缓存机制，因为数据库管理系统往往比操作系统更了解数据库中存放的数据，数据库管理系统可以提供一种更加有效的缓存机制来提高数据库中数据的存取性能
	优点：通过减少操作系统内核缓冲区和应用程序地址空间的数据拷贝次数，降低了对文件读取和写入时所带来的CPU的使用以及内存带宽的占用
	缺点：直接I/O的读写数据操作会造成磁盘的同步读写，导致进程执行缓慢。所以，应用程序使用直接I/O进行数据传输的时候通常会和使用异步I/O结合使用
		(异步IO：当访问数据的线程发出请求之后，线程会接着去处理其他事，而不是阻塞等待)
	
MMAP
	应用程序调用了mmap()之后，数据会先通过DMA拷贝到操作系统内核的缓冲区。接着，应用程序跟操作系统共享这个缓冲区。这样，操作系统内核和应用程序存储空间
		就不需要再进行任何的数据拷贝操作。
	也就是说内存映射文件MMAP只有一次页缓存的复制，读时从磁盘文件复制到页缓存，写时从页缓存flush到磁盘文件，默认30s。MMAP与操作系统的Pagecache打交道
	普通文件IO需要复制两次，内存映射文件mmap复制一次，普通文件IO是堆内操作，内存映射文件是堆外操作

SendFile
	sendfile()系统调用利用DMA引擎将文件中的数据拷贝到操作系统内核缓冲区中，然后数据被拷贝到与socket相关的内核缓冲区。接下来，DMA引擎将数据从内核socket缓冲区
		中拷贝到协议引擎。
	sendfile()系统调用不需要将数据拷贝或映射到应用程序地址空间，所以sendfile()只适用于应用程序地址空间不需要对所访问数据进行处理的情况。
		比如apache、nginx等web服务器使用sendfile传输静态文件。

总结
	常用的读文件方式read的过程是：
	磁盘->文件缓冲区->用户空间

	mmap是：
	磁盘->用户空间

	可以看到mmap少了一次内存copy。另外mmap可以通过多个用户进程映射到一个相同的文件（也可以是虚拟文件）达到共享内存，进程通信的效用。
		实现方法就是为两个进程的分配相同的页。

	再说sendfile
	nginx，apache都有开启sendfile的配置项，sendfile相对于传统的read+write+buffer的socket通信方式会高效一些

	传统过程：
	磁盘->文件缓冲区->用户空间->socket缓冲区->协议引擎

	sendfile：
	磁盘->文件缓冲区->socket缓冲区->协议引擎

	相对于mmap，sendfile少了内存映射的环节，如果传输很大的文件，内存映射的损耗可以忽略不计，但是如果传输的文件比较小，内存映射的损耗占比就会扩大

	splice：和sendfile()非常类似，用户应用程序必须拥有两个已经打开的文件描述符，一个用于表示输入设备，一个用于表示输出设备。与sendfile()不同的是，
		splice()允许任意两个文件之间互相连接，而并不只是文件到socket进行数据传输。

	对于从一个文件描述符发送数据到socket这种特例来说，一直都是使用sendfile()这个系统调用，而splice一直以来就只是一种机制，它并不仅限于sendfile()的功能。
		也就是说，sendfile()只是splice()的一个子集，在Linux 2.6.23中，sendfile()这种机制的实现已经没有了，但是这个API以及相应的功能还存在，只不过API以及相应的功能
		是利用了splice()这种机制来实现的。
		
	硬件设备跟内存通讯通过DMA完成，内存之间的copy是由cpu完成，减少内存copy可以解放cpu，提高系统负载

Kafka机制
	
	partition 顺序写入：mmap写入pagecache
		每一个Partition其实都是一个文件，收到消息后Kafka会把数据插入到文件末尾。消费者对每个Topic都有一个offset(存放ZK中)用来表示读取到了第几条数据。
		
		FileChannel fc = new RandomAccessFile("data.txt" , "rw").getChannel();
		long length = fc.size(); // 设置映射区域的开始位置，这是末尾写入的重点
		MappedByteBuffer mbb = fc.map(FileChannel.MapMode.READ_WRITE, length, 20);
		//由于要写入的字符串"Write in the end"占16字节，所以长度设置为20就足够了
		mbb.put("Write in the end".getBytes()); //写入新数据
		
		直接利用操作系统的Page来实现文件到物理内存的直接映射。完成映射之后你对物理内存的操作会被同步到硬盘上（操作系统在适当的时候）。也就是MappedByteBuffer类
		mmap的文件映射在full gc时才会进行释放。当close时，需要手动清除内存映射文件，反射调用sun.misc.Cleaner方法：MappedByteBuffer
		MappedByteBuffer未关闭导致慢磁盘访问。
		阿里的RocketMQ实现就是Kafka的java版本：MappedFile
	
	sendfile读取
		Kafka把所有的消息都存放在一个个文件中，当消费者需要数据的时候Kafka直接把“文件”发送给消费者。
		Kafka是用mmap作为文件读写方式的，它就是一个文件句柄，所以直接把它传给sendfile；偏移也好解决，用户会自己保持这个offset，每次请求都会发送这个offset
	
	总结
		Kafka速度的秘诀在于，它把所有的消息都变成一个的文件。通过mmap提高I/O速度，写入数据的时候它是末尾添加所以速度最优；读取数据的时候配合sendfile直接暴力输出。
	
	acks = 0; 不需要broker响应；
	acks = 1; 默认值，只要leader成功写入，producer就会收到来自broker服务端的成功响应；
	acks = -1 或 acks = all：等待ISR中的所有副本都成功写入后才能够收到来自服务端的成功响应；
	
2、负载均衡/故障转移：partition leader
	kafka只有partition leader进行读写，副本是不提供读写功能的，只是简单地从leader同步数据；
	负载均衡就是指让系统的负载根据一定的规则均衡地分配在所有参与工作的服务器上，从而最大限度的提升系统整体的运行效率。
	对于kafka来说就是，每台服务器broker都有均等的机会为kafka的客户提供服务，可以把负载分散到所有集群中的机器上。
	kafka通过智能化的分区领导者选举来实现负载均衡，kafka默认提供智能的leader选举算法，可在集群的所有机器上以均等机会分散各个partition的leader，从而整体上实现负载均衡。
	kafka的故障转移是通过使用会话机制实现的，每台kafka服务器启动后会以会话的形式把自己注册到zookeeper服务器上。一旦该服务器运转出现问题，
		与zookeeper的会话便不能维持从而超时失效，此时kafka集群会选举出另一台服务器来完全替代这台服务器继续提供服务。
	
3、伸缩性
	kafka服务器上的状态统一交由zookeeper保管，它只保存了很轻量级的内部状态，因此整个集群间维护状态一致性的代价是很低的。
	伸缩性是指向分布式系统中增加额外的计算资源比如CPU、内存、存储或带宽等时吞吐量提升的能力。
	如果一个CPU的运算能力是U，那么两个CPU的运算能力我们自然希望是2U，即可以线性的扩容计算能力，但是由于很多隐藏的“单点”瓶颈导致实际中几乎不可能达到。
	阻碍线性扩容的一个很常见的因素就是状态的保存，因为无论哪类分布式系统，集群中的每台服务器一定会维护很多内部状态，如果有服务器自己来保存这些状态信息，
	则必须要处理一致性的问题。相反，若服务器是无状态的，状态的保存和管理交由专门的协调服务来做比如zookeeper，那么整个集群的服务器之间就无需繁重的状态共享，
	就极大地降低了维护复杂度。倘若要扩容集群节点，只需简单的启动新的节点机器进行自动负载均衡就可以了。kafka正是采用上述思想，将每台kafka服务器上的状态
		统一交由zookeeper保管，扩展kafka集群时只需启动新的kafka服务器即可。说明：kafka服务器上并不是所有状态都不保存，之保存了很轻量级的内部状态，
		因此整个集群间维护状态一致性的代价很低。
	
4、kafka消息：紧凑的二进制格式，没有多余的bit位浪费；
	通过ByteBuffer来实现；
	kafka使用紧凑的二进制字节数组来保存字段，也就是没有多余的比特位浪费。通常的Java堆上内存分配，即使有重排各个字段在内存的布局以减少内存使用量的优化措施，
	但仍有部分字节用于补齐之用。同时，运行Java的操作系统通常都默认开启了页缓存机制，也就是说堆上保存的对象很可能在页缓存中还保留一份，这就造成了极大的资源浪费。
	kafka在消息设计时直接使用紧凑的二进制字节数组ByteBuffer而不是独立的对象，避开了繁重的java堆上内存分配。因此，我们至少能够访问多一倍的可用内存。还有一点，
	大量使用页缓存而非堆内存还有一个好处——数据不丢失，即当出现kafka broker进程崩溃时，堆内存上的数据也一并消失，但页缓存的数据依然存在。

	java字节对齐
		
		本文主要考虑正常情况下一个对象在堆上的内存占用情况：对于下面的特殊情况不作讨论
		1、某些情况下，JVM可能不会把对象存储在堆上：比如小的线程私有对象原则上会全部存储在栈或寄存器上，严格意义上说并不存在于java堆上
		2、对象的内存占用可能依赖于它当前的状态，比如说它的同步锁是否处于竞争状态、是否正处于垃圾回收阶段(这些额外的“系统”数据不一定存储在java堆上)
		
		在HotSpot虚拟机上，一个java对象的内存占用一般包括如下几部分：
		1、一个对象头部信息(包括几字节的基本元信息)
		2、原始类型字段的内存占用
		3、引用字段的内存占用
		4、对齐字节(padding):为了让每个对象的开始地址是字节的整数倍，减少对象指针占用的比特数，对象数据后面会添加一些“无用”的数据(字节)，以实现对齐，即保证最终的字节大小是8的倍数
		
		HotSpot虚拟机的对象头包含两部分信息：
		1、用于存储对象自身的运行时数据，这部分数据在32位和64位的虚拟机(未开启压缩指针)中分别为32bit和64bit。
		2、类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。注：如果java对象是一个数组，还必须包含用于记录数组长度的数据，
			因为java虚拟机可以从普通java对象的元数据信息确定对象的大小，但是从数组的元数据中却无法确定数组的大小。
	
		在HotSpot中，每个对象占用的内存大小是 8 字节的倍数。如果对象所需的内存大小(包括头信息和字段)不是 8 的倍数，则会向上取整到 8 的倍数。
		也就是说：
		1、一个空对象占用8字节
		2、只有一个 boolean 字段的类实例占 16 字节：头信息占 8 字节，boolean 占 1 字节，为了对齐达到 8 的倍数会额外占用 7 个字节
		3、包含 8 个 boolean 字段的实例也会占用 16 字节：头信息占用 8 字节，boolean 占用 8 字节；因为已经是 8 的倍数，不需要补充额外的数据来对齐
		4、一个包含 2 个 long 字段、3 个 int 字段、1 个 boolean 字段的对象将占用:
		头信息占 8 字节;
		2 个 long 字段占 16 字节(每个 long 字段占用 8 字节);
		3 个 int 字段占 12 字节(每个 int 字段占用 4 字节);
		1 个 boolean 字段占 1 个字节；

		为了对齐额外多 3 个字节(上面加起来是 37 字节，为满足对齐 8 的倍数 40)
		关于二维数组占用字节数计算：注意数组有一个不同的地方在于，它本身会有一个记录数组长度的int类型，占用4字节，本身又是一个对象，会占用8字节
		For example, let's consider a 10x10 int array. Firstly, the "outer" array has its 12-byte object header followed by space for the 10 elements. Those elements are object references to the 10 arrays making up the rows. That comes to 12+4*10=52 bytes, which must then be rounded up to the next multiple of 8, giving 56. Then, each of the 10 rows has its own 12-byte object header, 4*10=40 bytes for the actual row of ints, and again, 4 bytes of padding to bring the total for that row to a multiple of 8. So in total, that gives 11*56=616 bytes. That's a bit bigger than if you'd just counted on 10*10*4=400 bytes for the hundred "raw" ints themselves.


5、topic、partition、offset、replica、leader、flower；
	一个partition对应一个log，为了防止log过大，log会被分为多个logSegment，即多个小logSegment文件；
	一个log对应磁盘上的一个文件夹，每个logSegment对应一个日志文件，一个位移索引文件和一个时间索引文件，以及其他可能的文件，
	比如：.txnindex为后缀的事务索引文件。

	可以断言kafka中的一条消息其实就是一个<topic, partition,offset>三元组。

6、ISR：in-sync replica
	与leader replica保持同步的replica集合，只有这个集合里的replica才能被选举为leader，也只有该集合中所有replica都接收到了同一条消息，
	kafka才会将该消息置于“已提交”状态（这个其实要看ack配置的）；ISR集合是动态的；
	
7、HW：high watermark，高水位线
	leader的HW决定了哪些消息可以消费，也决定了flower的HW；
	
8、LEO：log end offset，下一条待写入消息的offset；

	消息写入leader后，follower副本会发送拉取请求来拉取消息3和消息4以进行消息同步；

9、索引：offset索引、时间索引、日志文件；
	
10、Producer：默认对key hash 取模分区；
	aks，0：直接发送，不管有没有持久化；all：ISR都持久化了才成功；1：leader持久化了即可；
	压缩：GZIP、LZ4、snappy；
	
	producer工作原理：
		说到producer，它的主要功能就是向某个topic的某个分区发送一条消息，所以它首先需要确定到底要向topic的哪个分区写入消息——这就是分区器做的事。
		kafka producer提供了一个默认的分区器，对于每条待发送的消息，如果该消息指定了key，那么partitioner会根据key的哈希值来选择目标分区；若这条消息没有指定key，
			则partitioner使用轮训的方式确认目标分区，从而最大限度的保证消息在所有分区上的均匀性。
		当然，producer提供了用户自行指定目标分区的API，即用户在消息发送时跳过partitioner直接指定要发送到的分区。另外，producer也允许用户实现自定义的分区策略而不使用默认的分区器。
		第二，确认了目标分区之后，producer要做的第二个事就是寻找这个分区对应的leader，也就是该分区leader副本所在的kafka broker。因此，在发送消息时，
			producer也就有了多种选择来实现消息发送（比如不等待任何副本的响应便返回成功、只是等待leader副本响应写入操作后再返回成功等）。
		producer简言之就是将用户待发送的消息封装成一个ProducerRecord对象，然后使用KafkaProducer.send方法进行发送。具体过程为：Producer首先使用一个线程
			（用户主线程，也即用户启动Producer的线程）将待发送的消息封装进一个ProducerRecord类实例，然后将其序列化之后发送给partitioner，再结合本地缓存的元数据信息
			由partitioner来确定目标分区后一同发送到位于producer程序中的一块内存缓冲区中。而KafkaProducer中的另一个专门的sender I/O线程则负责实时地从该缓冲区中提取出
			准备就绪的消息封装进一个批次（batch），统一发送给对应的broker。工作流程图如下图。

		自定义分区器，通过实现Partitioner接口实现；
		
	生产者拦截器：ProducerInterceptor
		onSend(ProducerRecord<K,V> record); //消息序列化和计算分区之前调用
		onAcknowledgement(RecordMetadata metadata, Exception exception); //在消息被应答（Acknowledgement）之前和消息发送失败时调用
		close(); //在关闭拦截器时执行一些资源的清理工作
		可以设置拦截器链，按顺序拦截
		
	主线程：KafkaProducer -> ProducerInterceptors -> Serializer -> Partitoner =>
	消息累加器RecordAccumulator：每个分区一个或多个ProducerBatch =>
	Sender线程：创建Request（有缓存Request） -> 提交给Selector准备发送 -> Kafka Cluster；
	
	整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和Sender线程。在主线程中由KafkaProducer创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存
	到消息累加器中RecordAccumulator。Sender线程负责从RecordAccumulator中获取消息并将其发送到kafka中。
	RecordAccmulator主要用来缓存消息以便Sender线程可以批量发送，进而减少网络传输的资源消耗以提升性能。可以设置参数buffer.memory，默认32MB。如果生产者发送消息的速度
	超过发送到服务器的速度，则会导致生产者空间不足，这时send()方法调用要么被阻塞，要么抛出异常，可以配置参数max.block.ms。
	一条或多条ProducerRecord组成ProducerBatch。生产者会在ProducerBatch被填满或等待时间超过linger.ms值时发送出去。

	leastLoadedNode：负载最小的broker节点，根据当前消息阻塞最少的节点（生产者发送消息，还没收到ack即代表消息阻塞）；
	
	更新集群元数据信息：生产者客户端会有相关策略（定时、需要查询元信息等），先挑选出leastLoadedNode，然后向这个Node发送MetadataRequest请求来获取具体的元数据信息。
	这个更新操作是Sender线程发起的。

	重要的生产者参数
		max.request.size：一条消息的最大多少，默认1MB。
		retries：发生异常，生产者重试发送消息的次数，默认为0，不重试。
		compression.type：消息压缩类型，默认none，不压缩；gzip、snappy、lz4。
		linger.ms：默认为0，生产者会在ProducerBatch被填满或等待时间超过linger.ms值时发送出去。
		request.timeout.ms：producer等待请求效应的最长时间，默认30000ms，请求超时重试。
		max.in.flight.requests.per.connection：该参数指定了生产者在收到服务器响应之前可以发送多少个消息。它的值越高，就会占用越多的内存，
			不过也会提升吞吐量。把它设为 1 可以保证消息是按照发送的顺序写入服务器的，即使发生了重试。

11、Consumer：
	老版本位移存在zk，新版本位移存在_consumer_offsets主题里；
	
	序列化：key.deserializer、value.deserializer，实现Serizlizer接口
		StringSerializer、Avro、JSON、Thrift、ProtoBuf、Protostuff等；
	
	kafka的consumer是用来读取消息的，且要能够同时读取多个topic的多个分区的消息。若要实现并行的消息读取，一种方法是使用多线程的方式，为每个要读取的分区都创建一个
		专有的线程去消费（这其实就是旧版本consumer采用的方式）；另一种方法是采用类似Linux I/O模型的poll或select等，使用一个线程来同时管理多个socket连接，
		即同时与多个broker通信实现消息的并行读取（这就是新版consumer最重要的设计改变）。
	新版本Java consumer是一个多线程或说是一个双线程的Java进程：创建KafkaConsumer的线程被称为用户主线程，同时consumer在后台会创建一个心跳线程。
		KafkaConsumer的poll方法在用户主线程中运行，而一旦consumer订阅了topic，所有的消费逻辑包括coordinator的协调、消费者组的rebalance以及数据的获取都会在
		主逻辑poll方法的一次调用中被执行。
	
	基于正则表达式订阅topic。
	
	消费组之间互不影响。
	每一个分区只能被一个消费组中的一个消费者所消费，所以8个分区，只会被一个分组中的8个消费者消费，第9个消费者会空闲。但一个消费者可以消费多个分区。
	
	kafka中的消费是基于拉模式的，通过不断轮询拉取消息；
	
	通过pause/resume方法，可以暂停/恢复对某个partition的消费；
	
	再均衡：指分区的所属权从一个消费者转移到另一消费者的行为，它为消费组具备高可用性和伸缩性提供保障，使我们可以既方便又安全地删除消费组内的消费者或往消费组内添加消费者。
	不过在再均衡发生期间，消费组内的消费者是无法读取信息的。也就是说，在再均衡发生期间的这一小段时间内，消费组会变得不可用。
	
	消费者拦截器：ConsumerInterceptor
	onConsume()//poll()方法返回之前调用
	onCommit()//提交位移之后调用
	close()//在关闭拦截器时执行一些资源的清理工作
	
	kafkaProducer是线程安全的，然而KafkaConsumer却不是线程安全的。
	
	多线程消费者：即每个partition一个线程，如果partition比较多，容易造成kafka长连接比较多，kafka长连接压力大，并且线程多了，consumer端系统开销也大。
	
	重要的消费者参数
	fetch.max.bytes：一次拉取请求中从kafka每个分区中拉取的最大数据量，默认50MB。如果一条消息大于50MB，也会被拉取到。
	
	消费者与分区的分配策略：partition.assignment.strategy
	RangeAssignor：消费者按分区范围平均分配，默认策略；按消费者名字的字典排序按分区范围平均分配，如果不够平均，排在前边的会多分配；
		consumer0：partition0、partition1;
		consumer1: partition2、partition3;
	RoundRobinAssignor：消费者、分区按字典排序，通过伦旭把分区依次分配给消费者。
		consumer0: partition0、partition2;
		consumer1: partition1、partition3;
	StickyAssignor：1、分区尽可能均匀；2、尽可能与上次分配的保持相同；
		比如一个consumer3挂掉了，RoundRobinAssignor会重新分配partition，但StickyAssignor会尽可能保持原来的consumer和partition关系，再分配，即有粘性；
	自定义分区：实现PartitionAssignor接口；
		
	_consumer_offsets：
	key：version + group + topic + partition
	value: version + offset + metadata + commit_timestamp + expire_timestamp
	
		
分区
	支持新增分区，但不支持减少分区；副本数则可以增加、减少；
	
	分区重分配，数据平衡；在要上线新节点或下线节点时，可以手工触发分区重分配。
	
	副本迁移复制，防止影响到线上业务，需要对复制限流，有相关配置参数可以控制；
	
	分区数并不是约多约好，当达到一定值后开始下降
	
日志存储
	每个partition有一个或多个logSegment文件；
	
	producer设置压缩方式，broker可以设置和producer相同的压缩方式，也可以不同，consumer无论设不设置，都可以解压；
	即使某段时间有压缩，某段时间不压缩，日志混存，consumer自动支持解压；
	
	日志清理策略：1、删除不符合条件的日志分段（默认策略）；2、相同key，不同value，只保留最后一个版本；
	
	日志删除：1、基于时间，默认保留7天；2、基于日志大小：只保留固定大小的日志，超过大小的日志段被删除（时间序列）；3、基于偏移量，小于某个偏移量的删除；
	
12、rebalance触发条件
	1、组成成员变更，减少、增加；崩溃：session.time.out
	2、topic数变更，正则表达式覆盖的数量；
	3、分区数发生变更；

13、Controller
	在Kafka集群中，会选举出一个broker来承担controller的角色，任意时刻，只能有一个controller；
	1、更新集群元数据信息；
	2、创建/删除topic；
	3、分区重分配；
	4、leader选举；
	5、broker加入集群、崩溃管理；
	
无消息丢失配置
	KafkaProducer.send方法仅仅把消息放入缓冲区中，由一个专属I/O线程负责从缓冲区中提取消息并封装进消息batch中，然后发送出去，而这个过程中存在着数据丢失的窗口：
		若I/O线程发送之前producer崩溃，则存在缓冲区中的消息全部丢失了。采用同步发送不会丢数据，但是性能会很差，实际场景中不推荐使用，因此最好能有一份配置，
		既使用异步方式还能有效避免数据丢失。
	
	1、producer端配置
		block.on.buffer.full =  true
		acks = all or –1
		retries = Integer.MAX_VALUE
		max.in.flight.requests.per.connection = 1
		使用带回调机制的send发送消息，即KafkaProducer.sent(record, callback)
		Callback逻辑中显式地立即关闭producer，使用close
		
	2、broker端参数配置
		unclean.leader.election.enable = false
		replication.factor = 3
		min.insync.replicas = 2 //ISR集合中最少的副本数
		replication.factor > min.insync.replicas
		enable.auto.commit = false
		
exactly-once
https://blog.csdn.net/zhangjun5965/article/details/78218169
	
14、生产者幂等性：配置参数enable.idempotence = true
	ProducerId、Sequence递增；ProducerId + partition -> sequence；
	1、ProducerID：在每个新的Producer初始化时，会被分配一个唯一的ProducerID，这个ProducerID对客户端使用者是不可见的。
	2、SequenceNumber：对于每个ProducerID，Producer发送数据的每个Topic和Partition都对应一个从0开始单调递增的SequenceNumber值。
	3、在每条消息中附带了PID（ProducerID）和SequenceNumber。相同的PID和SequenceNumber发送给Broker，而之前Broker缓存过之前发送的相同的消息，
		那么在消息流中的消息就只有一条，不会出现重复发送的情况。
	4、只能保证单个生产者会话中单分区的幂等，事务可以弥补这个缺陷。
	
15、事务
	事务要解决的问题：
		1、生产者向多个partition发送消息，要么全成功，要么全失败；
		2、事务更多是针对生产者的，消费者如果要实现全流程，需要checkpoint手段配合；
		
	过程
		1、transactionalId、producerId、partitionId、sequence、Epoch、TransactionCoordinator、Transaction Marker、isolation.level；
		2、生产者发送一条事务消息，像正常消息处理一样，但要把消息offset发送给TransactionCoordinator、_consume_offset；
		3、提交事务时，提交Transaction Marker控制信息；
		4、consumer对uncommit的消息依然能接收到，但不会消费（sdk层做了透明处理），会先缓存起来，等事务提交的时候，再从缓存中拿出来消费，否则丢掉；
		缓存消息的可靠性可以通过checkpoint和kafka offset重新消费实现；
	
15、事务
	事务可以保证对多个分区写入操作的原子性，多个操作要么全部成功，要么全部失败；
	对流式应用而言，一个典型的应用模式为consume-transform-produce，在这种模式下消费和生产并存，消费者可能在提交位移时异常导致重复消费，
	生产者也可能重复生产消息。kafka事务可以使应用程序将消费消息、生产消息、提交消费位移当作原子操作来处理，同时成功或失败，
	即使该生产、消费可能会跨分取。transactionId和pid一一对应；为了保证新的生产者启动后具有相同transactionalId的旧生产者能够立即失效，
	每个生产者通过transactionalId获取PID的同时，还会获取一个单调递增的producer epoch。如果使用同一个transactionalId开启两个生产者，
	那么前一个开启的生产者会报如下的错误。
	
	当用户使用 Kafka 的事务性时，Kafka 可以做到的保证：跨生产者、partition的消息幂等发送，并且事务性（要么全成功，要么失败）；
	1、跨生产者会话的消息幂等发送：即使中间故障，恢复后依然可以保持幂等性；
	2、跨生产者会话的事务恢复：如果一个应用实例挂了，启动的下一个实例依然可以保证上一个事务完成（commit 或者 abort）；
	3、跨多个 Topic-Partition 的幂等性写入，Kafka 可以保证跨多个 Topic-Partition 的数据要么全部写入成功，要么全部失败，不会出现中间状态。
	
	上面是从 Producer 的角度来看，那么如果从 Consumer 角度呢？Consumer端很难保证一个已经 commit 的事务的所有 msg 都会被消费，有以下几个原因：
	1、对于 compacted topic，在一个事务中写入的数据可能会被新的值覆盖；
	2、一个事务内的数据，可能会跨多个 log segment，如果旧的 segmeng 数据由于过期而被清除，那么这个事务的一部分数据就无法被消费到了；
	3、Consumer 在消费时可以通过 seek 机制，随机从一个位置开始消费，这也会导致一个事务内的部分数据无法消费；
	4、Consumer 可能没有订阅这个事务涉及的全部 Partition。
	
	消费者配置：
	isolation.level：read_uncommitted、read_committed
	read_uncommitted：consumer可以收到事务未提交的消息，但不会消费，只是缓存起来了。
	
	事务状态存储在_transaction_state主题中，包括事务日志；
	
	简单总结一下，关于 Kafka 事务性语义提供的保证主要以下三个：
	1、Atomic writes across multiple partitions.
	2、All messages in a transaction are made visible together, or none are.
	3、Consumers must be configured to skip uncommitted messages.
	
	Kafka 事务性的使用方法也非常简单，用户只需要在 Producer 的配置中配置 transactional.id，通过 initTransactions() 初始化事务状态信息，
	再通过 beginTransaction() 标识一个事务的开始，然后通过 commitTransaction() 或 abortTransaction() 对事务进行 commit 或 abort，示例如下所示：
	props.put("transaction.id", "value"); //应用程序提供唯一的transaction.id
	KafkaProducer producer = new KafkaProducer(props);
	producer.initTransactions();
	
	try {
		String msg = "matt test";
		producer.beginTransaction();
		producer.send(new ProducerRecord(topic, "0", msg.toString()));
		producer.send(new ProducerRecord(topic, "1", msg.toString()));
		producer.send(new ProducerRecord(topic, "2", msg.toString()));
		producer.commitTransaction();
	} catch (ProducerFencedException e1) {
		e1.printStackTrace();
		producer.close();
	} catch (KafkaException e2) {
		e2.printStackTrace();
		producer.abortTransaction();
	}
	producer.close();
	
	事务性要解决的问题
	1、在写多个 Topic-Partition 时，执行的一批写入操作，有可能出现部分 Topic-Partition 写入成功，部分写入失败（比如达到重试次数），
		这相当于出现了中间的状态，这并不是我们期望的结果；
	2、Producer 应用中间挂之后再恢复，无法做到 Exactly-Once 语义保证；

	1、如果启用事务性的话，涉及到多个 Topic-Partition 的写入时，这个事务操作要么会全部成功，要么会全部失败，不会出现上面的情况（部分成功、部分失败），
		如果有 Topic-Partition 无法写入，那么当前这个事务操作会直接 abort；
	2、其实应用做到端到端的 Exactly-Once，仅仅靠 Kafka 是无法做到的，还需要应用本身做相应的容错设计，以 Flink 为例，其容错设计就是 checkpoint 机制，
		作业保证在每次 checkpoint 成功时，它之前的处理都是 Exactly-Once 的，如果中间作业出现了故障，恢复之后，只需要接着上次 checkpoint 的记录做恢复即可，
		对于失败前那个未完成的事务执行回滚操作（abort）就可以了，这样的话就是实现了 Flink + Kafka 端到端的 Exactly-Once（这只是设计的思想，
		具体的实现后续会有文章详细解揭秘）。

	事务性实现的关键
	对于 Kafka 的事务性实现，最关键的就是其事务操作原子性的实现。
	1、关于这点，最容易想到的应该是引用 2PC 协议（它主要是解决分布式系统数据一致性的问题）中协调者的角色，它的作用是统计所有参与者的投票结果，
		如果大家一致认为可以 commit，那么就执行 commit，否则执行 abort：
	1.1、我们来想一下，Kafka 是不是也可以引入一个类似的角色来管理事务的状态，只有当 Producer 真正 commit 时，事务才会提交，
		否则事务会还在进行中（实际的实现中还需要考虑 timeout 的情况），不会处于完成状态；
	1.2、Producer 在开始一个事务时，告诉【协调者】事务开始，然后开始向多个 Topic-Partition 写数据，只有这批数据全部写完（中间没有出现异常），
		Producer 会调用 commit 接口进行 commit，然后事务真正提交，否则如果中间出现异常，那么事务将会被 abort（Producer 通过 abort 接口告诉【协调者】执行 abort 操作）；
	1.3、这里的协调者与 2PC 中的协调者略有不同，主要为了管理事务相关的状态信息，这就是 Kafka Server 端的 TransactionCoordinator 角色；

	2、有了上面的机制，是不是就可以了？很容易想到的问题就是 TransactionCoordinator 挂的话怎么办？TransactionCoordinator 如何实现高可用？
	2.1、TransactionCoordinator 需要管理事务的状态信息，如果一个事务的 TransactionCoordinator 挂的话，需要转移到其他的机器上，这里关键是在 事务状态信息如何恢复？ 
		也就是事务的状态信息需要很强的容错性、一致性；
	2.2、关于数据的强容错性、一致性，存储的容错性方案基本就是多副本机制，而对于一致性，就有很多的机制实现，其实这个在 Kafka 内部已经实现（不考虑数据重复问题），
		那就是 min.isr + ack 机制；
	2.3、分析到这里，对于 Kafka 熟悉的同学应该就知道，这个是不是跟 __consumer_offset 这个内部的 topic 很像，TransactionCoordinator 也跟 GroupCoordinator 类似，
		而对应事务数据（transaction log）就是 __transaction_state 这个内部 topic，所有事务状态信息都会持久化到这个 topic，TransactionCoordinator
		在做故障恢复也是从这个 topic 中恢复数据；

	3、有了上面的机制，就够了么？我们再来考虑一种情况，我们期望一个 Producer 在 Fail 恢复后能主动 abort 上次未完成的事务（接上之前未完成的事务），
		然后重新开始一个事务，这种情况应该怎么办？之前幂等性引入的 PID 是无法解决这个问题的，因为每次 Producer 在重启时，PID 都会更新为一个新值：
	3.1、Kafka 在 Producer 端引入了一个 TransactionalId 来解决这个问题，这个 txn.id 是由应用来配置的；
	3.2、TransactionalId 的引入还有一个好处，就是跟 consumer group 类似，它可以用来标识一个事务操作，便于这个事务的所有操作都能在一个地方
		（同一个 TransactionCoordinator）进行处理；
	
	4、再来考虑一个问题，在具体的实现时，我们应该如何标识一个事务操作的开始、进行、完成的状态？正常来说，一个事务操作是由很多操作组成的一个操作单元，
		对于 TransactionCoordinator 而言，是需要准确知道当前的事务操作处于哪个阶段，这样在容错恢复时，新选举的 TransactionCoordinator 才能恢复之前的状态：
	4.1、这个就是事务状态转移，一个事务从开始，都会有一个相应的状态标识，直到事务完成，有了事务的状态转移关系之后，TransactionCoordinator 
		对于事务的管理就会简单很多，TransactionCoordinator 会将当前事务的状态信息都会缓存起来，每当事务需要进行转移，就更新缓存中事务的状态
		（前提是这个状态转移是有效的）。

	总结一下，TransactionCoordinator 主要的功能有三个，分别是：
	1、处理事务相关的请求；
	2、维护事务的状态信息；
	3、向其他 Broker 发送 Transaction Marker 数据。

	Transaction Log（__transaction_state）
	1、在前面分析中，讨论过一个问题，那就是如果 TransactionCoordinator 故障的话应该怎么恢复？怎么恢复之前的状态？我们知道 Kafka 内部有一个事务 
	topic __transaction_state，一个事务应该由哪个 TransactionCoordinator 来处理，是根据其 txn.id 的 hash 值与 __transaction_state 的 partition 数取模得到，
	__transaction_state Partition 默认是50个，假设取模之后的结果是2，那么这个 txn.id 应该由 __transaction_state Partition 2 的 leader 来处理。
	2、对于 __transaction_state 这个 topic 默认是由 Server 端的 transaction.state.log.replication.factor 参数来配置，默认是3，如果当前 leader 故障，
	需要进行 leader 切换，也就是对应的 TransactionCoordinator 需要迁移到新的 leader 上，迁移之后，如何恢复之前的事务状态信息呢？
	3、正如 GroupCoordinator 的实现一样，TransactionCoordinator 的恢复也是通过 __transaction_state 中读取之前事务的日志信息，来恢复其状态信息，
	前提是要求事务日志写入做相应的不丢配置。这也是 __transaction_state 一个重要作用之一，用于 TransactionCoordinator 的恢复，__transaction_state 与 
	__consumer_offsets 一样是 compact 类型的 topic；

	Transaction Marker
	终于讲到了 Transaction Marker，这也是前面留的一个疑问，什么是 Transaction Marker？Transaction Marker 是用来解决什么问题的呢？
	Transaction Marker 也叫做 control messages，它的作用主要是告诉这个事务操作涉及的 Topic-Partition Set 的 leaders 当前的事务操作已经完成，
	可以执行 commit 或者 abort（Marker 主要的内容就是 commit 或 abort），这个 marker 数据由该事务的 TransactionCoordinator 来发送的。我们来假设一下：
	如果没有 Transaction Marker，一个事务在完成后，如何执行 commit 操作？（以这个事务涉及多个 Topic-Partition 写入为例）

	1、Transactional Producer 在进行 commit 时，需要先告诉 TransactionCoordinator 这个事务可以 commit 了（因为 TransactionCoordinator 记录这个事务对应的状态信息），
		然后再去告诉这些 Topic-Partition 的 leader 当前已经可以 commit，也就是 Transactional Producer 在执行 commit 时，至少需要做两步操作；
	2、在 Transactional Producer 通知这些 Topic-Partition 的 leader 事务可以 commit 时，这些 Topic-Partition 应该怎么处理呢？难道是 commit 时再把数据持久化到磁盘，
		abort 时就直接丢弃不做持久化？这明显是问题的，如果这是一个 long transaction 操作，写数据非常多，内存中无法存下，数据肯定是需要持久化到硬盘的，
		如果数据已经持久化到硬盘了，假设这个时候收到了一个 abort 操作，是需要把数据再从硬盘清掉？
	2.1、这种方案有一个问题是：已经持久化的数据是持久化到本身的日志文件，还是其他文件？如果持久化本来的日志文件中，那么 consumer 消费到一个未 commit 的数据怎么办？
		这些数据是有可能 abort 的，如果是持久化到其他文件中，这会涉及到数据多次写磁盘、从磁盘清除的操作，会影响其 server 端的性能；
	再看下如果有了 Transaction Marker 这个机制后，情况会变成什么样？
	2.1、首先 Transactional Producer 只需要告诉 TransactionCoordinator 当前事务可以 commit，然后再由 TransactionCoordinator 来向其涉及到的 Topic-Partition 的 
		leader 发送 Transaction Marker 数据，这里减轻了 Client 的压力，而且 TransactionCoordinator 会做一些优化，如果这个目标 Broker 涉及到多个事务操作，
		是可以共享这个 TCP 连接的；
	2.2、有了 Transaction Marker 之后，Producer 在持久化数据时就简单很多，写入的数据跟之前一样，按照条件持久化到硬盘（数据会有一个标识，标识这条或这批数据是不是
		事务写入的数据），当收到 Transaction Marker 时，把这个 Transaction Marker 数据也直接写入这个 Partition 中，这样在处理 Consumer 消费时，就可以根据 marker 
		信息做相应的处理。
	Transaction Marker 的数据格式如下，其中 ControlMessageType 为 0 代表是 COMMIT，为 1 代表是 ABORT：

	1、txn.id 是否可以被多 Producer 使用，如果有多个 Producer 使用了这个 txn.id 会出现什么问题？
		Producer1先启动，Producer2启动后，Producer1会报异常；
		
	2、TransactionCoordinator Fencing 和 Producer Fencing 分别是什么，它们是用来解决什么问题的？
		每个 TransactionCoordinator 都有其 CoordinatorEpoch 值，这个值就是对应 __transaction_statePartition 的 Epoch 值（每当 leader 切换一次，该值就会自增1）。
		Producer Epoch；新建一个Producer，Epoch都会加1；
		
	3、对于事务的数据，Consumer 端是如何消费的，一个事务可能会 commit，也可能会 abort，这个在 Consumer 端是如何体现的？
		在讲述这个问题之前，需要先介绍一下事务场景下，Consumer 的消费策略，Consumer 有一个 isolation.level 配置，这个是配置对于事务性数据的消费策略，
		有以下两种可选配置：
		1、read_committed: only consume non-­transactional messages or transactional messages that are already committed, in offset ordering.
		2、read_uncommitted: consume all available messages in offset ordering. This is the default value.
		简单来说就是，read_committed 只会读取 commit 的数据，而 abort 的数据不会向 consumer 显现，对于 read_uncommitted 这种模式，consumer 
		可以读取到所有数据（control msg 会过滤掉），这种模式与普通的消费机制基本没有区别，就是做了一个 check，过滤掉 control msg（也就是 marker 数据），
		这部分的难点在于 read_committed 机制的实现。
		
	4、对于一个 Topic，如果既有事务数据写入又有其他 topic 数据写入，消费时，其顺序性时怎么保证的？
		有了前面的分析，这个问题就很好回答了，顺序性还是严格按照 offset 的，只不过遇到 abort trsansaction 的数据时就丢弃掉，其他的与普通 Consumer 并没有区别。
		
	5、如果 txn.id 长期不使用，server 端怎么处理？
		Producer 在开始一个事务操作时，可以设置其事务超时时间（参数是 transaction.timeout.ms，默认60s），而且 Server 端还有一个最大可允许的事务操作超时时间
			（参数是 transaction.timeout.ms，默认是15min），Producer 设置超时时间不能超过 Server，否则的话会抛出异常。
		上面是关于事务操作的超时设置，而对于 txn.id，我们知道 TransactionCoordinator 会缓存 txn.id 的相关信息，如果没有超时机制，这个 meta 大小是无法预估的，
			Server 端提供了一个 transaction.id.expiration.ms 参数来配置这个超时时间（默认是7天），如果超过这个时间没有任何事务相关的请求发送过来，
			那么 TransactionCoordinator 将会使这个 txn.id 过期。
			
	6、PID Snapshot 是做什么的？是用来解决什么问题？
		
****************************************************************************************************************************

事务实现过程
		查找事务协调者

		生产者会首先发起一个查找事务协调者的请求(FindCoordinatorRequest)。协调者会负责分配一个PID给生产者。类似于消费组的协调者。

		获取produce ID
		在知道事务协调者后，生产者需要往协调者发送初始化pid请求(initPidRequest)。这个请求分两种情况：

		●)不带transactionID
		这种情况下直接生成一个新的produce ID即可，返回给客户端

		●带transactionID
		这种情况下，kafka根据transactionalId获取对应的PID，这个对应关系是保存在事务日志中（上图2a）。这样可以确保相同的TransactionId返回相同的PID，用于恢复或者终止之前未完成的事务。

		启动事务
		生产者通过调用beginTransaction接口启动事务，此时只是内部的状态记录为事务开始，但是事务协调者认为事务开始只有当生产者开始发送第一条消息才开始。

		消费和生产配合过程( The consume-transform-produce loop)
		这一步是消费和生成互相配合完成事务的过程，其中涉及多个请求：
		●4.1增加分区到事务请求 AddPartitionToTxnRequest
		当生产者有新分区要写入数据，则会发送AddPartitionToTxnRequest到事务协调者。协调者会处理请求，主要做的事情是更新事务元数据信息，并把信息写入到事务日志中（事务Topic）,如图4.1a。

		●4.2生产请求 ProduceRequests
		生产者通过调用send接口发送数据到分区，这些请求新增pid，epoch和sequence number字段，如图 4.2a。

		●4.3增加消费offset到事务
		生产者通过新增的snedOffsets ToTransaction接口，会发送某个分区的Offset信息到事务协调者。协调者会把分区信息增加到事务中。
		●4.4事务提交offset请求
		当生产者调用事务提交offset接口后，会发送一个TxnOffsetCommitRequest请求到消费组协调者，消费组协调者会把offset存储在__consumer-offsets Topic中。协调者会根据请求的PID和epoch验证生产者是否允许发起这个请求。 消费offset只有当事务提交后才对外可见。

		提交或回滚事务(Committing or Aborting a Transaction)
		用户通过调用commitTransaction或abortTranssaction方法提交或回滚事务。

		●EndTxnRequest
		当生产者完成事务后，客户端需要显式调用结束事务或者回滚事务。前者会使得消息对消费者可见，后者会对生产数据标记为Abort状态，使得消息对消费者不可见。无论是提交或者回滚，都是发送一个EndTnxRequest请求到事务协调者，写入PREPARE_COMMIT或者PREPARE_ABORT信息到事务记录日志中(5.1a)。

		●WriteTxnMarkerRequest
		这个请求是事务协调者向事务中每个TopicPartition的Leader发送的。每个Broker收到请求后会写入COMMIT(PID)或者ABORT(PID)控制信息到数据日志中(5.2a)。

		这个信息用于告知消费者当前消息是哪个事务，消息是否应该接受或者丢弃。而对于未提交消息，消费者会缓存该事务的消息直到提交或者回滚。

		这里要注意，如果事务也涉及到__consumer_offsets，即该事务中有消费数据的操作且将该消费的Offset存于__consumer_offsets中，Transaction Coordinator也需要向该内部Topic的各Partition的Leader发送WriteTxnMarkerRequest从而写入COMMIT(PID)或COMMIT(PID)控制信息(5.2a 左边)。

		●写入最终提交或回滚信息
		当提交和回滚信息写入数据日子后，事务协调者会往事务日志中写入最终的提交或者终止信息以表示事务已经完成(图5.3)，此时大部分于事务有关系的消息都可以被删除（通过标记后面在日志压缩时会被移除），我们只需要保留事务ID以及其时间戳即可。

******************************************************************************************

Producer --> Broker --> Consumer

持久化：磁盘；

Consumer：pull；

At Most Once
At Latest Once
Exact Once

At Most Once
	Producer；
	Broker；
	Consumer：commit offset first + business logic；
	
At Latest Once
	Producer：sync + ack；
	Broker：可靠性/一致性/分区容错：proxyos/raft/ISR/...；
	Consumer：business logic + commit offset；
	
Exact Once
	幂等、全局唯一ID；
	Producer：pid + sequenceNumber(topic + partion)；
	Broker；
	Consumer：business Logic；
	
事务

性能太差
	Producer：异步、缓存、压缩；
	Broker：顺序写、页缓存、零拷贝；
	Consumer：partion；
	
以下概念非kafka原生概念，kafka原生不支持：
	过期队列TTL：定时过期，类似redis、zookeper；
	延时队列：延时消费；
	死信队列：一些消息无法正常消费，或重试多次消费都没确认，会进入死信队列；
	重试队列：当前无法正常消费的，可以丢到重试队列，等下重试消费；
	
leader Epoch：HW, LEO
	A(leader)、B(follower)
	1、当前B有2条消息m1、m2，LEO = 2， HW = 1；A通过FetchRequest同步m1、m2，LEO = 2，HW = 1；
	2、B在确认ISR follower同步了消息后，设置HW = 2；
	3、之后A向B发送请求以拉取消息，FetchRequest请求中带上了A的LEO信息，B在收到请求之后更新了自己的HW为2；B中没有更多消息，但还是在延时一段时间后返回FetchResponse，
		并包含了HW信息：最后A根据FetchResponse中的HW信息更新自己的HW为2；
	4、如果在步骤2、3之间A宕机了，那么在A重启之后会根据之前HW进行日志截断，这样便会把m2删掉，此时A只剩下m1这条信息，之后A再向B发送FetchRequest请求拉取消息；
	5、此时若B再宕机，那么A就会被选举为新的leader，B恢复后成为follower，由于follower副本HW不能比leader副本的HW高，所以还会做一次日志截断，从此将HW调整为1，这样m2消息就
		彻底丢失了。
	解决方案：
	1、等follower HW都更新完后再更新leader的HW？这样会增加多一轮的FetchRequest/FetchResponse延迟，自然不妥当；
	2、收到leader的fetchResponse之前不要截断日志？假设A为leader，且有两条消息m1、m2，HW和LEO都是2，B为follower，有一条消息m1，并且HW、LEO都为1；假设A/B同时挂掉，然后B第一个恢复并成为leader，
		之后B写入消息m3，并将LEO和HW更新为2，此时A也恢复过来了，发送fetchRequest至B，此时A的HW也正好为2，那么就可以不做任何调整了，如此A中保留了m2而B中没有，B中新增了m3而
		A也同步不到，这样A/B的数据不一致了。
		
	最终解决方案：leader Epoch
	初始值为0，leader没变更一次就加1（比如宕机），相当于为leader增加一个版本号，与此同时，每个副本中还会增设一个矢量<LeaderEpoch=>StartOffset>，StartOffset代表当前
	LeaderEpoch下写入的第一条消息的偏移量。每个副本的Log下都有一个leader-epoch-checkpoint文件，在发生leader epoch变更时，会将对应的矢量对追加到这个文件中。
	如果LeaderEpoch相同，follower就不截断日志，否则就查找follower保存的leader epoch对应的startOffset截断日志；
