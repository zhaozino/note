1、高吞吐量/低延时
	虽然kafka会持久化所有数据到磁盘，但本质上每次写入都只是把数据写入到操作系统的页缓存（page cache）中，然后由操作系统自行决定什么时候把
		页缓存中的数据写回磁盘。
	
	这样设计的优势：
		1、页缓存时在内存中分配的，所以写入速度非常快；
		2、kafka不必与底层的文件系统打交道，所有繁琐的I/O操作都交给操作系统处理；
		3、kafka写入操作采用追加写入（append）的方式，避免了磁盘随机写操作；
		4、日志压缩，发送压缩；
		
	kafka在读取消息时首先从页缓存中读，如果命中，便把消息直接发送到网络的socket上，这个过程就是利用linux平台的sendfile系统调用做到的，
		即：零拷贝技术（Zero Copy）；
	
	零拷贝：传统的Linux操作系统中的I/O接口是依托于数据拷贝来实现的，一个I/O操作会将同一份数据多次拷贝，数据传输过程中还涉及内核态与用户态
		的上下文切换，CPU的开销非常大，极大地限制了OS高效进行数据传输的能力。
		零拷贝技术：在内核驱动程序处理I/O数据的时候，它不再需要进行上下文的切换，节省了内核缓冲区与用户态应用程序缓冲区之间的数据拷贝，
		同时，它利用直接存储器访问技术（Direct Memory Access，DMA）执行I/O操作，因此也避免了OS内核缓冲区之间的数据拷贝，即零拷贝；
		linux提供的sendfile系统调用实现了这种零拷贝技术，而kafka的消息消费机制使用的就是sendfile，通过java的FileChannel.transferTo方法实现的；
	
	Kafka server读消息传送到消费者
	传统：硬盘—>内核buf—>用户buf—>socket相关缓冲区—>协议引擎
	sendfile：硬盘->内核buf->socket相关缓冲区->协议引擎
	
	
2、负载均衡/故障转移：partition leader
	kafka只有partition leader进行读写，副本是不提供读写功能的，只是简单地从leader同步数据；
	
3、伸缩性
	kafka服务器上的状态统一交由zookeeper保管，它只保存了很轻量级的内部状态，因此整个集群间维护状态一致性的代价是很低的。
	
4、kafka消息：紧凑的二进制格式，没有多余的bit位浪费；
	通过ByteBuffer来实现；

5、topic、partition、offset、replica、leader、flower；

	一个partition对应一个log，为了防止log过大，log会被分为多个logSegment，即多个小logSegment文件；
	一个log对应磁盘上的一个文件夹，每个logSegment对应一个日志文件，一个位移索引文件和一个时间索引文件，以及其他可能的文件，
	比如：.txnindex为后缀的事务索引文件。

6、ISR：in-sync replica
	与leader replica保持同步的replica集合，只有这个集合里的replica才能被选举为leader，也只有该集合中所有replica都接收到了同一条消息，
	kafka才会将该消息置于“已提交”状态（这个其实要看ack配置的）；ISR集合是动态的；
	
7、HW：high watermark，高水位线
	leader的HW决定了哪些消息可以消费，也决定了flower的HW；
	
8、LEO：log end offset，下一条待写入消息的offset；

9、索引：offset索引、时间索引、日志文件；
	
10、Producer：默认对key hash 取模分区；
	aks，0：直接发送，不管有没有持久化；all：ISR都持久化了才成功；1：leader持久化了即可；
	压缩：GZIP、LZ4、snappy；

11、Consumer：
	老版本位移存在zk，新版本位移存在_consumer_offsets主题里；
	
12、rebalance触发条件
	1、组成成员变更，减少、增加；崩溃：session.time.out
	2、topic数变更，正则表达式覆盖的数量；
	3、分区数发生变更；

13、Controller
	在Kafka集群中，会选举出一个broker来承担controller的角色，任意时刻，只能有一个controller；
	1、更新集群元数据信息；
	2、创建/删除topic；
	3、分区重分配；
	4、leader选举；
	5、broker加入集群、崩溃管理；
	
exactly-once
https://blog.csdn.net/zhangjun5965/article/details/78218169
	
14、生产者幂等性
	Pid、Sequence递增；pid + partition > sequence；
	1、ProducerID：在每个新的Producer初始化时，会被分配一个唯一的ProducerID，这个ProducerID对客户端使用者是不可见的。
	2、SequenceNumber：对于每个ProducerID，Producer发送数据的每个Topic和Partition都对应一个从0开始单调递增的SequenceNumber值。
	3、在每条消息中附带了PID（ProducerID）和SequenceNumber。相同的PID和SequenceNumber发送给Broker，而之前Broker缓存过之前发送的相同的消息，
		那么在消息流中的消息就只有一条，不会出现重复发送的情况。
	
15、事务
	事务可以保证对多个分区写入操作的原子性，多个操作要么全部成功，要么全部失败；
	对流式应用而言，一个典型的应用模式为consume-transform-produce，在这种模式下消费和生产并存，消费者可能在提交位移时异常导致重复消费，
	生产者也可能重复生产消息。kafka事务可以使应用程序将消费消息、生产消息、提交消费位移当作原子操作来处理，同时成功或失败，
	即使该生产、消费可能会跨分取。transactionId和pid一一对应；
	
	当用户使用 Kafka 的事务性时，Kafka 可以做到的保证：
	1、跨会话的幂等性写入：即使中间故障，恢复后依然可以保持幂等性；
	2、跨会话的事务恢复：如果一个应用实例挂了，启动的下一个实例依然可以保证上一个事务完成（commit 或者 abort）；
	3、跨多个 Topic-Partition 的幂等性写入，Kafka 可以保证跨多个 Topic-Partition 的数据要么全部写入成功，要么全部失败，不会出现中间状态。
	
	上面是从 Producer 的角度来看，那么如果从 Consumer 角度呢？Consumer 端很难保证一个已经 commit 的事务的所有 msg 都会被消费，有以下几个原因：
	1、对于 compacted topic，在一个事务中写入的数据可能会被新的值覆盖；
	2、一个事务内的数据，可能会跨多个 log segment，如果旧的 segmeng 数据由于过期而被清除，那么这个事务的一部分数据就无法被消费到了；
	3、Consumer 在消费时可以通过 seek 机制，随机从一个位置开始消费，这也会导致一个事务内的部分数据无法消费；
	4、Consumer 可能没有订阅这个事务涉及的全部 Partition。
	
	简单总结一下，关于 Kafka 事务性语义提供的保证主要以下三个：
	1、Atomic writes across multiple partitions.
	2、All messages in a transaction are made visible together, or none are.
	3、Consumers must be configured to skip uncommitted messages.
	
	Kafka 事务性的使用方法也非常简单，用户只需要在 Producer 的配置中配置 transactional.id，通过 initTransactions() 初始化事务状态信息，
	再通过 beginTransaction() 标识一个事务的开始，然后通过 commitTransaction() 或 abortTransaction() 对事务进行 commit 或 abort，示例如下所示：
	KafkaProducer producer = new KafkaProducer(props);
	producer.initTransactions();
	
	try {
		String msg = "matt test";
		producer.beginTransaction();
		producer.send(new ProducerRecord(topic, "0", msg.toString()));
		producer.send(new ProducerRecord(topic, "1", msg.toString()));
		producer.send(new ProducerRecord(topic, "2", msg.toString()));
		producer.commitTransaction();
	} catch (ProducerFencedException e1) {
		e1.printStackTrace();
		producer.close();
	} catch (KafkaException e2) {
		e2.printStackTrace();
		producer.abortTransaction();
	}
	producer.close();
	
	事务性要解决的问题
	1、在写多个 Topic-Partition 时，执行的一批写入操作，有可能出现部分 Topic-Partition 写入成功，部分写入失败（比如达到重试次数），
		这相当于出现了中间的状态，这并不是我们期望的结果；
	2、Producer 应用中间挂之后再恢复，无法做到 Exactly-Once 语义保证；

	1、如果启用事务性的话，涉及到多个 Topic-Partition 的写入时，这个事务操作要么会全部成功，要么会全部失败，不会出现上面的情况（部分成功、部分失败），
		如果有 Topic-Partition 无法写入，那么当前这个事务操作会直接 abort；
	2、其实应用做到端到端的 Exactly-Once，仅仅靠 Kafka 是无法做到的，还需要应用本身做相应的容错设计，以 Flink 为例，其容错设计就是 checkpoint 机制，
		作业保证在每次 checkpoint 成功时，它之前的处理都是 Exactly-Once 的，如果中间作业出现了故障，恢复之后，只需要接着上次 checkpoint 的记录做恢复即可，
		对于失败前那个未完成的事务执行回滚操作（abort）就可以了，这样的话就是实现了 Flink + Kafka 端到端的 Exactly-Once（这只是设计的思想，
		具体的实现后续会有文章详细解揭秘）。

	事务性实现的关键
	对于 Kafka 的事务性实现，最关键的就是其事务操作原子性的实现。
	1、关于这点，最容易想到的应该是引用 2PC 协议（它主要是解决分布式系统数据一致性的问题）中协调者的角色，它的作用是统计所有参与者的投票结果，
		如果大家一致认为可以 commit，那么就执行 commit，否则执行 abort：
	1.1、我们来想一下，Kafka 是不是也可以引入一个类似的角色来管理事务的状态，只有当 Producer 真正 commit 时，事务才会提交，
		否则事务会还在进行中（实际的实现中还需要考虑 timeout 的情况），不会处于完成状态；
	1.2、Producer 在开始一个事务时，告诉【协调者】事务开始，然后开始向多个 Topic-Partition 写数据，只有这批数据全部写完（中间没有出现异常），
		Producer 会调用 commit 接口进行 commit，然后事务真正提交，否则如果中间出现异常，那么事务将会被 abort（Producer 通过 abort 接口告诉【协调者】执行 abort 操作）；
	1.3、这里的协调者与 2PC 中的协调者略有不同，主要为了管理事务相关的状态信息，这就是 Kafka Server 端的 TransactionCoordinator 角色；

	2、有了上面的机制，是不是就可以了？很容易想到的问题就是 TransactionCoordinator 挂的话怎么办？TransactionCoordinator 如何实现高可用？
	2.1、TransactionCoordinator 需要管理事务的状态信息，如果一个事务的 TransactionCoordinator 挂的话，需要转移到其他的机器上，这里关键是在 事务状态信息如何恢复？ 
		也就是事务的状态信息需要很强的容错性、一致性；
	2.2、关于数据的强容错性、一致性，存储的容错性方案基本就是多副本机制，而对于一致性，就有很多的机制实现，其实这个在 Kafka 内部已经实现（不考虑数据重复问题），
		那就是 min.isr + ack 机制；
	2.3、分析到这里，对于 Kafka 熟悉的同学应该就知道，这个是不是跟 __consumer_offset 这个内部的 topic 很像，TransactionCoordinator 也跟 GroupCoordinator 类似，
		而对应事务数据（transaction log）就是 __transaction_state 这个内部 topic，所有事务状态信息都会持久化到这个 topic，TransactionCoordinator
		在做故障恢复也是从这个 topic 中恢复数据；

	3、有了上面的机制，就够了么？我们再来考虑一种情况，我们期望一个 Producer 在 Fail 恢复后能主动 abort 上次未完成的事务（接上之前未完成的事务），
		然后重新开始一个事务，这种情况应该怎么办？之前幂等性引入的 PID 是无法解决这个问题的，因为每次 Producer 在重启时，PID 都会更新为一个新值：
	3.1、Kafka 在 Producer 端引入了一个 TransactionalId 来解决这个问题，这个 txn.id 是由应用来配置的；
	3.2、TransactionalId 的引入还有一个好处，就是跟 consumer group 类似，它可以用来标识一个事务操作，便于这个事务的所有操作都能在一个地方
		（同一个 TransactionCoordinator）进行处理；
	
	4、再来考虑一个问题，在具体的实现时，我们应该如何标识一个事务操作的开始、进行、完成的状态？正常来说，一个事务操作是由很多操作组成的一个操作单元，
		对于 TransactionCoordinator 而言，是需要准确知道当前的事务操作处于哪个阶段，这样在容错恢复时，新选举的 TransactionCoordinator 才能恢复之前的状态：
	4.1、这个就是事务状态转移，一个事务从开始，都会有一个相应的状态标识，直到事务完成，有了事务的状态转移关系之后，TransactionCoordinator 
		对于事务的管理就会简单很多，TransactionCoordinator 会将当前事务的状态信息都会缓存起来，每当事务需要进行转移，就更新缓存中事务的状态
		（前提是这个状态转移是有效的）。

	总结一下，TransactionCoordinator 主要的功能有三个，分别是：
	1、处理事务相关的请求；
	2、维护事务的状态信息；
	3、向其他 Broker 发送 Transaction Marker 数据。

	Transaction Log（__transaction_state）
	1、在前面分析中，讨论过一个问题，那就是如果 TransactionCoordinator 故障的话应该怎么恢复？怎么恢复之前的状态？我们知道 Kafka 内部有一个事务 
	topic __transaction_state，一个事务应该由哪个 TransactionCoordinator 来处理，是根据其 txn.id 的 hash 值与 __transaction_state 的 partition 数取模得到，
	__transaction_state Partition 默认是50个，假设取模之后的结果是2，那么这个 txn.id 应该由 __transaction_state Partition 2 的 leader 来处理。
	2、对于 __transaction_state 这个 topic 默认是由 Server 端的 transaction.state.log.replication.factor 参数来配置，默认是3，如果当前 leader 故障，
	需要进行 leader 切换，也就是对应的 TransactionCoordinator 需要迁移到新的 leader 上，迁移之后，如何恢复之前的事务状态信息呢？
	3、正如 GroupCoordinator 的实现一样，TransactionCoordinator 的恢复也是通过 __transaction_state 中读取之前事务的日志信息，来恢复其状态信息，
	前提是要求事务日志写入做相应的不丢配置。这也是 __transaction_state 一个重要作用之一，用于 TransactionCoordinator 的恢复，__transaction_state 与 
	__consumer_offsets 一样是 compact 类型的 topic；

	Transaction Marker
	终于讲到了 Transaction Marker，这也是前面留的一个疑问，什么是 Transaction Marker？Transaction Marker 是用来解决什么问题的呢？
	Transaction Marker 也叫做 control messages，它的作用主要是告诉这个事务操作涉及的 Topic-Partition Set 的 leaders 当前的事务操作已经完成，
	可以执行 commit 或者 abort（Marker 主要的内容就是 commit 或 abort），这个 marker 数据由该事务的 TransactionCoordinator 来发送的。我们来假设一下：
	如果没有 Transaction Marker，一个事务在完成后，如何执行 commit 操作？（以这个事务涉及多个 Topic-Partition 写入为例）

	1、Transactional Producer 在进行 commit 时，需要先告诉 TransactionCoordinator 这个事务可以 commit 了（因为 TransactionCoordinator 记录这个事务对应的状态信息），
		然后再去告诉这些 Topic-Partition 的 leader 当前已经可以 commit，也就是 Transactional Producer 在执行 commit 时，至少需要做两步操作；
	2、在 Transactional Producer 通知这些 Topic-Partition 的 leader 事务可以 commit 时，这些 Topic-Partition 应该怎么处理呢？难道是 commit 时再把数据持久化到磁盘，
		abort 时就直接丢弃不做持久化？这明显是问题的，如果这是一个 long transaction 操作，写数据非常多，内存中无法存下，数据肯定是需要持久化到硬盘的，
		如果数据已经持久化到硬盘了，假设这个时候收到了一个 abort 操作，是需要把数据再从硬盘清掉？
	2.1、这种方案有一个问题是：已经持久化的数据是持久化到本身的日志文件，还是其他文件？如果持久化本来的日志文件中，那么 consumer 消费到一个未 commit 的数据怎么办？
		这些数据是有可能 abort 的，如果是持久化到其他文件中，这会涉及到数据多次写磁盘、从磁盘清除的操作，会影响其 server 端的性能；
	再看下如果有了 Transaction Marker 这个机制后，情况会变成什么样？
	2.1、首先 Transactional Producer 只需要告诉 TransactionCoordinator 当前事务可以 commit，然后再由 TransactionCoordinator 来向其涉及到的 Topic-Partition 的 
		leader 发送 Transaction Marker 数据，这里减轻了 Client 的压力，而且 TransactionCoordinator 会做一些优化，如果这个目标 Broker 涉及到多个事务操作，
		是可以共享这个 TCP 连接的；
	2.2、有了 Transaction Marker 之后，Producer 在持久化数据时就简单很多，写入的数据跟之前一样，按照条件持久化到硬盘（数据会有一个标识，标识这条或这批数据是不是
		事务写入的数据），当收到 Transaction Marker 时，把这个 Transaction Marker 数据也直接写入这个 Partition 中，这样在处理 Consumer 消费时，就可以根据 marker 
		信息做相应的处理。
	Transaction Marker 的数据格式如下，其中 ControlMessageType 为 0 代表是 COMMIT，为 1 代表是 ABORT：

	1、txn.id 是否可以被多 Producer 使用，如果有多个 Producer 使用了这个 txn.id 会出现什么问题？
		Producer1先启动，Producer2启动后，Producer1会报异常；
		
	2、TransactionCoordinator Fencing 和 Producer Fencing 分别是什么，它们是用来解决什么问题的？
		每个 TransactionCoordinator 都有其 CoordinatorEpoch 值，这个值就是对应 __transaction_statePartition 的 Epoch 值（每当 leader 切换一次，该值就会自增1）。
		Producer Epoch；新建一个Producer，Epoch都会加1；
		
	3、对于事务的数据，Consumer 端是如何消费的，一个事务可能会 commit，也可能会 abort，这个在 Consumer 端是如何体现的？
		在讲述这个问题之前，需要先介绍一下事务场景下，Consumer 的消费策略，Consumer 有一个 isolation.level 配置，这个是配置对于事务性数据的消费策略，
		有以下两种可选配置：
		1、read_committed: only consume non-­transactional messages or transactional messages that are already committed, in offset ordering.
		2、read_uncommitted: consume all available messages in offset ordering. This is the default value.
		简单来说就是，read_committed 只会读取 commit 的数据，而 abort 的数据不会向 consumer 显现，对于 read_uncommitted 这种模式，consumer 
		可以读取到所有数据（control msg 会过滤掉），这种模式与普通的消费机制基本没有区别，就是做了一个 check，过滤掉 control msg（也就是 marker 数据），
		这部分的难点在于 read_committed 机制的实现。
		
	4、对于一个 Topic，如果既有事务数据写入又有其他 topic 数据写入，消费时，其顺序性时怎么保证的？
		有了前面的分析，这个问题就很好回答了，顺序性还是严格按照 offset 的，只不过遇到 abort trsansaction 的数据时就丢弃掉，其他的与普通 Consumer 并没有区别。
		
	5、如果 txn.id 长期不使用，server 端怎么处理？
		Producer 在开始一个事务操作时，可以设置其事务超时时间（参数是 transaction.timeout.ms，默认60s），而且 Server 端还有一个最大可允许的事务操作超时时间
			（参数是 transaction.timeout.ms，默认是15min），Producer 设置超时时间不能超过 Server，否则的话会抛出异常。
		上面是关于事务操作的超时设置，而对于 txn.id，我们知道 TransactionCoordinator 会缓存 txn.id 的相关信息，如果没有超时机制，这个 meta 大小是无法预估的，
			Server 端提供了一个 transaction.id.expiration.ms 参数来配置这个超时时间（默认是7天），如果超过这个时间没有任何事务相关的请求发送过来，
			那么 TransactionCoordinator 将会使这个 txn.id 过期。
			
	6、PID Snapshot 是做什么的？是用来解决什么问题？
		
****************************************************************************************************************************

Producer --> Broker --> Consumer

持久化：磁盘；

Consumer：pull；

At Most Once
At Latest Once
Exact Once

At Most Once
	Producer；
	Broker；
	Consumer：commit offset first + business logic；
	
At Latest Once
	Producer：sync + ack；
	Broker：可靠性/一致性/分区容错：proxyos/raft/ISR/...；
	Consumer：business logic + commit offset；
	
Exact Once
	幂等、全局唯一ID；
	Producer：pid + sequenceNumber(topic + partion)；
	Broker；
	Consumer：business Logic；
	
事务

性能太差
	Producer：异步、缓存、压缩；
	Broker：顺序写、页缓存、零拷贝；
	Consumer：partion；



















