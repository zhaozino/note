
1、write-ahead log，也称HLog；
2、MemStore；
HBase的默认方式是把写入动作记录在这两个地方，以保证数据持久化。只有当这两个地方的变化信息都写入并确认后，才认为写动作完成；

MemStore是内存里的写入缓冲区，HBase中数据在永久写入硬盘之前在这里积累。当MemStore填满后，其中的数据会刷写到硬盘，生成一个HFile。
	HFile是HBase使用的底层存储格式。HFile对应于列族，一个列族可以有多个HFile，但一个HFile不能存储多个列族的数据。在集群的每个节点
	上，每个列族有一个MemStore。
	
每台HBase服务器有一个WAL，这台服务器上的所有表（和它们的列族）共享这个WAL。

HBase读动作必须重新衔接持久化到硬盘上的HFile和内存中MemStore里的数据。HBase在读操作上使用了LRU（最近最少使用算法）缓存技术；这种
	缓存也叫BlockCache，和MemStore在一个JVM堆里；每个列族都有自己的BlockCache。

BlockCache中的Block是HBase从硬盘完成一次读取的数据单位。HFile物理存放形式是一个Block的序列外加这些Block的索引。这意味着，从HBase里
	读取一个Block需要先在索引上查找一次该Block然后从硬盘读出。Block是建立索引的最小数据单位，也是从硬盘读取的最小数据单位。Block大小
	按照列族设定，默认值是64KB。根据使用场景你可能会调大或调小该值。如果主要用于随机查询，你可能需要细粒度的Block索引，小一点的Block
	更好一些。Block变小会导致索引变大，进而消耗更多内存。如果你经常执行顺序扫描，一次读取多个Block，大一点的Block更好一些。Block变大
	意味着索引项变少，索引变小，因此节省内存。

从HBase中读取一行，首先会检查MemStore等待修改的队列，然后检查BlockCache看包含该行的Block是否最近被访问过，最后访问硬盘上的对应HFile。

Delete命令并不立即删除内容，实际上，它只是给记录打上删除的标记。就是说，针对那个内容的一条新“墓碑”记录写入进来，作为删除的标记。墓碑记录
	用来标志删除的内容不能再Get和Scan命令中返回结果。因为HFile文件是不能改变的，直到执行一次大合并，这些墓碑记录才会被处理，被删除记录
	占用的空间才会释放。

合并分为两种：大合并、小合并。两者将会重整存储在HFile里的数据。小合并把多个小HFile合并生成一个大HFile。因为读出一条完整的行可能引用很多
	文件，限制HFile的数量对于读性能很重要。执行合并时，HBase读出已有的多个HFile的内容，把记录写入一个新文件。然后，把新文件设置为激活状态，
	删除构成这个新文件的老文件。HBase根据文件的号码和大小决定合并哪些文件。小合并设计出发点是轻微影响HBase的性能，所以涉及的HFile的数量
	有上限，这些都可以设置。
	
大合并将处理给定region的一个列族的所有HFile。大合并完成后，这个列族的所有HFile合并成一个文件。可以从Shell中手工触发整个表（或者特定region）
	的大合并。这个动作相当耗费资源，不要经常使用。另一方面，小合并是轻量级的，可以频繁发生。大合并是HBase清理被删除记录的唯一机会。因为我们
	不能保证被删除的记录和墓碑标记记录在一个HFile里面。大合并是唯一的机会，HBase可以确保同时访问到两种记录。

ResultScanner.next(int)
	
Get、Put、Delete、Scan、Increment。

RegionServer和HDFS DataNode典型情况下并列配置在同一物理硬件上，虽说这不是必需的。RegionServer本质上是HDFS客户端，在上面存储/访问数据。
	主（master）进程分配region给RegionServer，每个RegionServer一般托管多个region；
	
HBase中有两个特殊的表，-ROOT-和.META.，用来查找各种表的region位置在哪里。-ROOT-和.META.像HBase中其他表一样也会切分成region。-ROOT-和
	.META.都是特殊的表，但-ROOT-比.META.更特殊一些，-ROOT-永远不会切分超过一个region。META.和其他表一样可以按需要切分成许多region。

当客户端应用要访问某行时，它先找-ROOT-表，查找什么地方可以找到负责某行的region。-ROOT-指向.META.表的region，那里有这个问题的答案。
	.META.表由入口地址组成，客户端应用使用这个入口地址判断哪一个RegionServer托管待查找的region。从zookeeper中可以查到-ROOT-表在哪个
	RegionServer上。
	
布隆过滤器
	数据块索引提供了一种有效的方法，在访问一个特定的行时用来查找应该读取的HFile的数据块，但它的效用很有限；
布隆过滤器允许对存储在数据块的数据做一个反向测试。当某行被请求时，先检查布隆过滤器，看看该行是否不在这个数据块中。布隆过滤器要么确定
	回答该行不在，要么回答它不知道。布隆过滤器也可以应用到行中的单元上。当访问某列限定符时先使用同样的反向测试。
布隆过滤器也不是没有代价的。存储这个额外的索引层会占用额外的空间。布隆过滤器随着它们索引的对象数据增长而增长，所以行级布隆过滤器比限定符
	级布隆过滤器占用空间要少。当空间不是问题时，它们可以帮助你榨干系统的性能潜力。
	
预装过滤器
	1、行过滤器：RowFilter
		基于行键执行精确匹配、子字符串匹配、正则表达式匹配，过滤掉不匹配的数据。
		
	2、前缀过滤器：PrefixFilter
		这是RowFilter的一种特例。它是基于行健的前缀值进行过滤。
	
	3、限定符过滤器：QualifierFilter
		类似RowFilter，不同之处是它用来匹配列限定符而不是行健。
		
	4、值过滤器：ValueFilter
		针对的是单元值过滤。
		
	5、时间戳过滤器：TimestampsFilter
		针对返回给客户端的时间版本进行更细粒度的控制；
		
协处理器：observer、endpoint
	你可以使用定制过滤器把应用逻辑推到集群上，但是过滤器被局限在单行的内容上。为了在HBase里执行行数据上的计算，你被迫依靠Hadoop MapReduce
		或者依靠客户端代码来读取、修改和写回数据到HBase。
	随着协处理器的引入，我们可以把任意计算逻辑推到托管数据的HBase节点上。这种代码跨越所有RegionServer并行运行。这个特性把HBase集群
		从水平扩展存储系统转变为高效的、分布式的数据存储和数据处理系统。类似于数据库的存储过程。
		
observer：位于客户端和HBase之间，在这个过程发生时修改数据访问。你可以在每个Get命令后运行一个observer，修改返回给客户端的结果。
	
RegionObserver：这种observer钩在数据访问和操作阶段。所有标准的数据操作命令都可以被pre-hooks和post-hooks拦截。它也对region内部
	操作开放pre-hooks和post-hooks，例如，刷写MemStore和拆分region。RegionObserver运行在region上，因此同一个RegionServer上可以运行多个
	RegionObserver。
	
WALObserver：预写日志也支持observer卸处理器。唯一可用的钩子是pre-WAL和post-WAL写事件。和RegionObserver不同，WALObserver运行在RegionServer
	的环境里。
	
MasterObserver：为了钩住DDL事件，如表的创建或模式修改，HBase提供了MasterObserver，这种observer运行在Master节点上。

endpoint协处理器：endpoint是HBase的一种通用扩展。当endpoint安装在集群上时，它扩展了HBase RPC协议，对客户端应用开放了新方法。就像observer，
	endpoint在RegionServer上执行，紧挨着你的数据。类似于存储过程，从客户端看，调用一个endpoint协处理器类似调用其他HBase命令，只是其功能
	建立在定义协处理器的定制代码上。通常先创建请求对象，然后把它传给HtableInterface在集群上执行，最后收集结果。
	
	最基本的是，endpoint可以用来实现分散聚合算法。AggregateImplementation实例在托管数据的节点上计算得到部分结果，然后AggregationClient
	在客户端进程里计算得到最终结果。重点，在客户端聚合；

LSM
数据首先写入内存中MemoryStore（一般为了防止down机数据丢失，还要把数据写入WAL中），且数据是在MemoryStore范围内是有序的，MemoryStore写满后会刷到磁盘上，生成一个文件HFile，这个HFile文件是有序的；随着时间推移，会生成多个HFile，且单个HFile是有序的，用户读数据时，会同时读取MemoryStore和HFile，以命中数据，或把获取的数据合并排序后返回给用户；
后边会有合并动作，把多个小的HFile合成一个大的HFile，以提高读取性能；如果有删除数据的操作，则照样会在HFile中追加一条删除标记的记录，因为HFile只能追加，不能修改，根据索引，会同时命中原数据和删除标记的墓碑数据；
	
1、MemStore和HFile中的数据都是按照 RowKey 的字典序排序；
2、Hbase在读写数据时需要通过RowKey找到对应的Region；

在HBase中，一个Region就相当于一个数据分片，每个Region都有StartRowKey和StopRowKey（用来表示 Region存储的RowKey的范围），HBase表里面的数据是按照RowKey来分散存储到不同的Region里面的。

而将数据记录均衡的分散到不同的Region中避免热点现象就是RowKey最主要的作用。

什么是热点现象？

在实际操作中，当大量请求访问HBase集群的一个或少数几个节点，造成少数RegionServer的读写请求过多，负载过大，而其他RegionServer负载却很小，这样就造成热点现象。

通过RowKey的优化，避免热点现象

反转（Reversing）
顾名思义它就是把固定长度或者数字格式的 rowkey进行反转，反转分为一般数据反转和时间戳反转，其中以时间戳反转较常见。

适用场景：

初步设计出的RowKey在数据分布上不均匀，但RowKey尾部的数据却呈现出了良好的随机性（注意：随机性强代表经常改变，没意义，但分布较好），此时，可以考虑将RowKey的信息翻转，或者直接将尾部的bytes提前到RowKey的开头。反转可以有效的使RowKey随机分布，但是反转后有序性肯定就得不到保障了，因此它牺牲了RowKey的有序性。

加盐（Salting）
不同于密码学里得加盐方法，RowKey的加盐原理是在原RowKey的前面添加固定长度的随机数，也就是给RowKey分配一个随机前缀使它和之前的RowKey的开头不同。

适用场景：

设计的RowKey虽有意义的，但是数据类似，随机性比较低，反转也没法保证随机性，这样就没法根据RowKey分配到不同的Region里，这时候就可以使用加盐的方式了。

需要注意随机数要能保障数据在所有Regions间的负载均衡，也就是说分配的随机前缀的种类数量应该和你想把数据分散到的那些region的数量一致。只有这样，加盐之后的rowkey才会根据随机生成的前缀分散到各个region中，避免了热点现象。

哈希（Hashing）
这里的哈希是基于RowKey的完整或部分数据进行Hash，而后将哈希后的值完整替换或部分替换原RowKey的前缀部分。这里说的hash常用的有MD5、sha1、sha256 或 sha512 等算法。

适用场景：

其实哈希和加盐的适用场景类似，但是由于加盐方法的前缀是随机数，用原rowkey查询时不方便，因此出现了哈希方法，由于哈希是使用各种常见的算法来计算出的前缀，因此哈希既可以使负载分散到整个集群，又可以轻松读取数据。

RowKey设计原则

HBase提出了四点RowKey的设计原则：长度原则、唯一原则、排序原则，散列原则。

长度原则
RowKey本质上是一个二进制码的流，可以是任意字符串，最大长度为64kb，实际应用中一般为10-100byte，以byte[]数组形式保存，一般设计成定长。官方建议越短越好，不要超过16个字节。

唯一原则
由于RowKey用来唯一标识一行记录，所以必须在设计上保证RowKey的唯一性。

需要注意，由于HBase中数据存储的格式是Key-Value对格式，所以如果向HBase中同一张表插入相同RowKey的数据，则原先存在的数据会被新的数据给覆盖掉（和HashMap效果相同）。

排序原则
HBase会把RowKey按照ASCII进行自然有序排序，所以反过来在设计RowKey的时候可以根据这个特点来设计完美的RowKey，利用好这个特性就是排序原则。

散列原则
设计出的RowKey需要能够均匀的分布到各个RegionServer上。比如设计RowKey的时候，当Rowkey 是按时间戳的方式递增，就不要将时间放在二进制码的前面，可以将 Rowkey 的高位作为散列字段，由程序循环生成，可以在低位放时间字段，这样就可以提高数据均衡分布在每个Regionserver实现负载均衡的几率。
	
	
不预分取，region会自动分裂；

在HBase中，表会被划分为1...n个Region，被托管在RegionServer中。Region二个重要的属性:StartKey与EndKey表示这个Region维护的rowKey范围，当我们要读/写数据时，如果rowKey落在某个start-end key范围内，那么就会定位到目标region并且读/写到相关的数据。

1、由于业务数据一般都是从小到大增长的，根据上面hbase的region规则，就会出现“热点写”问题，随着系统的运营，数据总是会往最大的start-key所在的region里写，因为我们的rowkey总是会比之前的大，并且hbase的是按升序方式排序的。所以写操作总是被定位到无上界的那个region中。
2、其次，由于写热点，我们总是往最大start-key的region写记录，之前分裂出来的region不会再被写数据，有点被打进冷宫的赶脚，它们都处于半满状态，这样的分布也是不利的。
如果在写比较频率的场景下，数据增长快，split的次数也会增多，由于split是比较耗时耗资源的，所以我们并不希望这种事情经常发生。

看到这些缺点，我们知道，在集群的环境中，为了得到更好的并行性，我们希望有好的load blance，让每个节点提供的请求处理都是均等的。我们也希望，region不要经常split，因为split会使server有一段时间的停顿，如何能做到呢？
随机散列与预分区

随机散列与预分区：二者结合起来，是比较完美的，预分区一开始就预建好了一部分region,这些region都维护着自已的start-end keys，再配合上随机散列，写数据能均等地命中这些预建的region，就能解决上面的那些缺点，大大地提高了性能。
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
